

---
title: "分词"
date: 2022-01-30
categories:
- python
tags:
- 分词

# keywords:
# - 队列


thumbnailImagePosition: left
# thumbnailImage: //d1u9biwaxjngwg.cloudfront.net/elements-showcase/vintage-140.jpg
---

<!--more-->

### 分词 

#### 1：分词是什么？

```
官方对分词的解释：分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。

说直白点，就是把一句话，按照原有的顺序切分成若干个字或者词的片段的组合的过程。

例如  原始句子 我爱你中国

分词后变成了 我 爱  你 中国

与原始句子比，整体顺序未变 一个完整的句子被划分成了多个片段，片段与片段之间有明显的分割符号 ，这就是最简单的分词形式。
```

#### 2：分词的使用场景

```
一般来说 如果你有搜索 的场景那么一定要用到 分词 如果自己本身的数据 则可以借助 ES等数据库来完成
如果是别人的数据 想要获得更好的 数据 就需要分词了
比如现有的 词 或句子将其 通过分词 得到更多的词 再通过搜索引擎 获取更多的数据等
```

#### 3：python 分词的操作

```
在此我列举一下 我使用过的两个分词 工具 一个 是百度 的lac （https://github.com/baidu/lac/） github 地址
一个就是常用的jieba (https://github.com/fxsjy/jieba)  github 地址


https://github.com/baidu/lac/blob/master/python/README.md  官方 python 版本的文档

from LAC import LAC

# 装载词语重要性模型
lac = LAC(mode='rank')

# 单个样本输入，输入为Unicode编码的字符串
text = u"LAC是个优秀的分词工具"
rank_result = lac.run(text)

# 批量样本输入, 输入为多个句子组成的list，平均速率会更快
texts = [u"LAC是个优秀的分词工具", u"百度是一家高科技公司"]
rank_result = lac.run(texts)

【批量样本】：lac_result = [
                    ([百度, 是, 一家, 高科技, 公司], [ORG, v, m, n, n]),
                    ([LAC, 是, 个, 优秀, 的, 分词, 工具], [nz, v, q, a, u, n, n])
                ]
词性和专名类别标签集合如下表，其中我们将最常用的4个专名类别标记为大写的形式：

标签	  含义	  标签	  含义	  标签	   含义	    标签	  含义
n	     普通名词	 f	  方位名词	 s	    处所名词	 nw	   作品名
nz	   其他专名	 v	  普通动词	 vd	    动副词	    vn	  名动词
a	     形容词	 ad	   副形词	  an	   名形词	    d	    副词
m	     数量词	  q	   量词	     r	    代词	     p	   介词
c	     连词	    u	    助词	    xc	  其他虚词	   w	  标点符号
PER	   人名	    LOC	  地名	    ORG	  机构名	    TIME	  时间

具体 其他需求 可以继续查看官方文档

import jieba

# jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持
strs=["我来到北京清华大学","乒乓球拍卖完了","中国科学技术大学"]
for str in strs:
seg_list = jieba.cut(str) # 使用paddle模式
print("Paddle Mode: " + '/'.join(list(seg_list)))

seg_list = jieba.cut("我来到北京清华大学")
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学")
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print(", ".join(seg_list))

Paddle Mode: 我/来到/北京/清华大学
Paddle Mode: 乒乓球/拍卖/完/了
Paddle Mode: 中国/科学技术/大学
Full Mode: 我/ 来到/ 北京/ 清华大学
Default Mode: 我/ 来到/ 北京/ 清华大学
他, 来到, 了, 网易, 杭研, 大厦
小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造

开发者可以指定 自定义的词典 以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率
这个 跟 es 中 的自定义词典是一个意思 
用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径
具体的细节 可以查看文档

```



